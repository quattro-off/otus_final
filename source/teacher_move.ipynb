{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module\n",
    "\n",
    "from stable_baselines3 import DDPG, SAC, TD3, DQN, PPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.utils import polyak_update\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os, shutil, sys\n",
    "sys.path.append(os.path.abspath('./env'))\n",
    "sys.path.append(os.path.abspath('./common'))\n",
    "\n",
    "from env.env_simple_move import HumanMoveSimpleAction\n",
    "from env.env_move_simple_v2 import MoveSimpleActionV2\n",
    "from env.env_move_simple_v3 import MoveSimpleActionV3\n",
    "from env.env_move_sector import HumanMoveSectorAction\n",
    "from common.nets import MoveNet\n",
    "\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "TZ = timezone('Europe/Moscow')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MoveSimpleActionV3()\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.shape[0]\n",
    "n_hidden = 256\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HumanMoveSectorAction(object_ignore=True)\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.shape[0]\n",
    "n_hidden = 256\n",
    "lr = 0.003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "     \n",
    "model = TD3('MlpPolicy', env, action_noise=action_noise, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPG('MlpPolicy', env, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(model, state, action, q_v):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=DEVICE)\n",
    "    action = torch.tensor(action, dtype=torch.float32, device=DEVICE)\n",
    "    q_v = torch.tensor([q_v], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    action_by_net = model.actor.mu.forward(state)\n",
    "    loss = criterion(action_by_net, action)\n",
    "\n",
    "    model.actor.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.actor.optimizer.step()\n",
    "\n",
    "    critic_loss = None\n",
    "    qvalue_input = torch.cat([state, action], dim=0)\n",
    "    q_by_net = tuple(q_net(qvalue_input) for q_net in model.critic.q_networks)\n",
    "    critic_loss = sum(criterion(current_q, q_v) for current_q in q_by_net)\n",
    "\n",
    "    model.critic.optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    model.critic.optimizer.step()\n",
    "\n",
    "    polyak_update(model.actor.parameters(), model.actor_target.parameters(), model.tau)\n",
    "    polyak_update(model.critic.parameters(), model.critic_target.parameters(), model.tau)\n",
    "\n",
    "\n",
    "\n",
    "def get_action(model, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "    action = model.actor.mu.forward(state)\n",
    "    action = action.detach().cpu().numpy()[0]\n",
    "    action = np.clip(action, -1.0, 1.0)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение с учителем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 10000\n",
    "total_reward_episode = np.zeros(n_episode)\n",
    "\n",
    "for n in tqdm(range(n_episode)):\n",
    "    state, _ = env.reset()\n",
    "    action = get_action(model, state)\n",
    "    teach_action = env.teach_action()\n",
    "    next_state, reward, is_done, is_break, _ = env.step(teach_action)\n",
    "    update(model, state, teach_action, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode = 10\n",
    "total_reward_episode = np.zeros(n_episode)\n",
    "\n",
    "for n in tqdm(range(n_episode)):\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    is_done = False\n",
    "    is_break = False\n",
    "    sum_reward = 0\n",
    "\n",
    "    while not is_done and not is_break:\n",
    "        #action = model.get_action(state)\n",
    "        action, _ = model.predict(state)\n",
    "        next_state, reward, is_done, is_break, _ = env.step(action)\n",
    "        sum_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    total_reward_episode[n] = sum_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title('Зависимость вознаграждения в эпизоде от времени')\n",
    "plt.xlabel('Эпизод')\n",
    "plt.ylabel('Полное вознаграждение')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int( datetime.now(TZ).strftime(\"%H%M%S\") )\n",
    "#env_render = HumanMoveSimpleAction(seed=seed, target_point_rand=True, render_mode = 'human')\n",
    "env_render = HumanMoveSectorAction(seed=seed, target_point_rand=True, object_ignore=True, render_mode = 'human')\n",
    "total_reward = 0.\n",
    "step_reward = []\n",
    "observation, _ = env_render.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not terminated and not truncated:\n",
    "    #action = model.get_action(observation)\n",
    "    action, _ = model.predict(observation)\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env_render.step(action)\n",
    "    total_reward += reward\n",
    "    step_reward.append(reward)\n",
    "\n",
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.actor_target.state_dict(), './teached/move_simple_v3_TD3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = torch.load('./teached/move_simple_v3_TD3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scr_model = torch.jit.trace(model.policy, example_inputs=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_model = torch.jit.script(model.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(m=scr_model,f='./teached/script_move_simple_v3_TD3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./teached/move_simple_v3_TD3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./teached/sector_move_TD3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./teached/sector_move_DDPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_model = TD3.load('./teached/simple_move_TD3', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_model = TD3.load('./364/DDPG/model', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_render = HumanMoveSimpleAction(render_mode = 'human',target_point_rand=True)\n",
    "seed = int( datetime.now(TZ).strftime(\"%H%M%S\") )\n",
    "env_render = HumanMoveSectorAction(seed=seed, target_point_rand=False, object_ignore=True, render_mode = 'human')\n",
    "total_reward = 0.\n",
    "step_reward = []\n",
    "observation, _ = env_render.reset(seed=seed)\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not terminated and not truncated:\n",
    "    action, _ = read_model.predict(observation)\n",
    "    observation, reward, terminated, truncated, _ = env_render.step(action)\n",
    "    total_reward += reward\n",
    "    step_reward.append(reward)\n",
    "\n",
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_render.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_conda_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
