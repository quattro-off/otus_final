{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from typing import Tuple\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath('./env'))\n",
    "sys.path.append(os.path.abspath('./common'))\n",
    "\n",
    "from env.env_move_sector_v3  import HumanMoveSectorActionV3\n",
    "from env.env_move_ray_v3  import HumanMoveRayActionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HumanMoveSectorActionV3(target_point_rand=True, object_ignore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OnnxableSB3Policy(th.nn.Module):\n",
    "    def __init__(self, policy: BasePolicy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        # NOTE: Preprocessing is included, but postprocessing\n",
    "        # (clipping/inscaling actions) is not,\n",
    "        # If needed, you also need to transpose the images so that they are channel first\n",
    "        # use deterministic=False if you want to export the stochastic policy\n",
    "        # policy() returns `actions, values, log_prob` for PPO\n",
    "        return self.policy(observation, deterministic=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = PPO.load(\"env_Wall_MoveRay3_RanTP_exp_0112_115555/model\", device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_policy = OnnxableSB3Policy(model.policy)\n",
    "\n",
    "observation_size = model.observation_space.shape\n",
    "dummy_input = th.randn(1, *observation_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"teached/move_ray_v3_ppo.onnx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "th.onnx.export(\n",
    "    onnx_policy,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    opset_version=17,\n",
    "    input_names=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Load and test with onnx\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_move_wall import HumanMoveRayAroundWallActionV3\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n",
    "\n",
    "TZ = timezone('Europe/Moscow')\n",
    "\n",
    "seed = int( datetime.now(TZ).strftime(\"%H%M%S\") )\n",
    "print(seed)\n",
    "\n",
    "#env = HumanMoveSectorActionV3(render_mode = 'human', target_point_rand=False, object_ignore=True, seed=seed)\n",
    "env = HumanMoveRayAroundWallActionV3(render_mode = 'human',\n",
    "                                target_point_rand = True,\n",
    "                                object_locate = 'build',\n",
    "                                wall_border=0,\n",
    "                                tree_count=0,\n",
    "                                line_angle=-90,\n",
    "                                seed=seed,\n",
    "                           )\n",
    "\n",
    "total_reward = 0.\n",
    "step_reward = []\n",
    "angle_step_reward = []\n",
    "speed_step_reward = []\n",
    "view_step_reward = []\n",
    "stoper_step_reward = []\n",
    "obstacle_reward_stop = []\n",
    "obstacle_reward_move = []\n",
    "a_speed_x = []\n",
    "a_speed_y = []\n",
    "a_speed_a = []\n",
    "\n",
    "observation, info = env.reset(seed=seed)\n",
    "\n",
    "\n",
    "for tick in range(1800):\n",
    "\n",
    "\n",
    "\n",
    "    ort_sess = ort.InferenceSession(onnx_path)\n",
    "    actions, values, log_prob = ort_sess.run(None, {\"input\": [observation]})\n",
    "    \n",
    "    a_speed_x.append(actions[0][0])\n",
    "    a_speed_y.append(actions[0][1])\n",
    "    a_speed_a.append(actions[0][2])\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(actions[0])\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        step_reward.append(reward)\n",
    "        total_reward += reward\n",
    "        print('BREAK',terminated, truncated)\n",
    "        break\n",
    "    \n",
    "    step_reward.append(reward)\n",
    "    total_reward += reward\n",
    "\n",
    "    rews = env.get_rewards()\n",
    "    angle_step_reward.append(rews['angle_reward'])\n",
    "    speed_step_reward.append(rews['speed_reward'])\n",
    "    view_step_reward.append(rews['view_reward'] )\n",
    "    stoper_step_reward.append(rews['stoped_reward'])\n",
    "    obstacle_reward_stop.append(rews['object_stop'])\n",
    "    obstacle_reward_move.append(rews['object_move'])\n",
    "\n",
    "print(len(step_reward))\n",
    "print(total_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(step_reward[:-5])\n",
    "plt.plot(angle_step_reward,color='b')\n",
    "plt.plot(speed_step_reward,color='g')\n",
    "plt.plot(view_step_reward, color='pink')\n",
    "plt.plot(stoper_step_reward, color='yellow')\n",
    "plt.plot(obstacle_reward_stop,color='black')\n",
    "plt.plot(obstacle_reward_move,color='r')\n",
    "plt.legend([\n",
    "    #'total',\n",
    "    'angle',\n",
    "    'speed',\n",
    "    'view',\n",
    "    'stoper',\n",
    "    'obstacle_stop',\n",
    "    'obstacle_free'\n",
    "    ])\n",
    "plt.title('Вознаграждения за шаг')\n",
    "plt.xlabel('Шаг')\n",
    "plt.ylabel('Вознаграждения')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_speed_x,color='b')\n",
    "plt.plot(a_speed_y,color='g')\n",
    "plt.plot(a_speed_a, color='pink')\n",
    "plt.legend([\n",
    "    'speed_x',\n",
    "    'speed_y',\n",
    "    'angle',\n",
    "    ])\n",
    "plt.title('Управляющие действия')\n",
    "plt.xlabel('Шаг')\n",
    "plt.ylabel('Действие')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check that the predictions are the same\n",
    "with th.no_grad():\n",
    "    print(model.policy(th.as_tensor(observation), deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_conda_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
